# -*- coding: utf-8 -*-
"""AQEDFashionMNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuI2wthL9sljHG5OUswtJ8rEEsxmU4vg

# Fashion-MNIST Classification

### Dataset: Fashion-MNIST
- 10 classes: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot
- 28×28 grayscale images
- More challenging than MNIST digits

### Innovation:
- Claude API generates optimized quantum encoding parameters
- 5-layer data re-uploading quantum feature map
- Hybrid quantum-classical classification
"""

!pip -q install qiskit qiskit-machine-learning scikit-learn numpy matplotlib seaborn pylatexenc requests tensorflow

import numpy as np
import json
import time
import requests
from typing import List

from sklearn.datasets import fetch_openml
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.manifold import TSNE

from qiskit import QuantumCircuit
from qiskit.quantum_info import Statevector
from qiskit.circuit.library import ZZFeatureMap, ZFeatureMap
from qiskit_machine_learning.kernels import FidelityQuantumKernel

# Visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Set style for all plots
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

print("All imports successful!")

# ⚠️ SET YOUR CLAUDE API KEY
CLAUDE_API_KEY = ""  # Replace with your actual API key

if CLAUDE_API_KEY == "your-api-key-here":
    print("⚠️ Please set your Claude API key!")
else:
    print("✅ API key set!")

"""## Claude API - Generate Quantum Parameters for Fashion-MNIST"""

class ClaudeQuantumDesigner:
    """Uses Claude API to generate optimized quantum encoding parameters."""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.api_url = "https://api.anthropic.com/v1/messages"
        self.model = "claude-sonnet-4-20250514"

    def generate_config(self, n_qubits: int, n_features: int, dataset: str = "Fashion-MNIST") -> dict:
        """Generate optimized configuration for the dataset."""

        prompt = f"""You are a quantum machine learning expert. Generate ONE optimized parameter set for a quantum feature map.

Dataset: {dataset}
- 10 classes of clothing items
- More texture/pattern variation than digit MNIST
- Requires stronger feature discrimination

System: {n_qubits} qubits, {n_features} PCA features.

The quantum circuit uses 5-layer data re-uploading:
- Layer 1: RY primary encoding (captures intensity)
- Layer 2: RZ product features (captures correlations)
- Layer 3: Entanglement + RX (creates superposition)
- Layer 4: RY re-upload with shifted features (captures edges)
- Layer 5: RZ final phase encoding

For Fashion-MNIST, optimize for texture discrimination.

Return ONLY a JSON object with:
- ry1_scale: float (0.8-1.8) - Layer 1 scale
- rz1_scale: float (0.3-1.0) - Layer 2 scale
- rx_scale: float (0.5-1.5) - Layer 3 scale
- ry2_scale: float (0.6-1.4) - Layer 4 scale
- rz2_scale: float (0.2-0.8) - Layer 5 scale
- entangle_pattern: "linear" or "circular" or "full"
- feature_shift: int (1-{n_qubits//2}) - shift for re-upload

Return ONLY JSON, no explanation."""

        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01"
        }

        data = {
            "model": self.model,
            "max_tokens": 500,
            "messages": [{"role": "user", "content": prompt}]
        }

        try:
            print("Calling Claude API for Fashion-MNIST config...")
            response = requests.post(self.api_url, headers=headers, json=data, timeout=60)
            response.raise_for_status()

            result = response.json()
            content = result['content'][0]['text'].strip()

            if "```" in content:
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]

            config = json.loads(content)
            print(f"✅ Claude generated config: {config}")
            return config

        except Exception as e:
            print(f"⚠️ API error: {e}, using optimized default")
            return self._default_config()

    def _default_config(self) -> dict:
        """Optimized default for Fashion-MNIST."""
        return {
            "ry1_scale": 1.4,
            "rz1_scale": 0.6,
            "rx_scale": 1.0,
            "ry2_scale": 1.1,
            "rz2_scale": 0.5,
            "entangle_pattern": "circular",
            "feature_shift": 3
        }

print("ClaudeQuantumDesigner ready!")

"""## AQED Quantum Feature Map (5-Layer Architecture)"""

class AQEDQuantumFeatureMap:
    """
    AQED 5-Layer Data Re-uploading Quantum Feature Map.
    Optimized for Fashion-MNIST texture features.
    """

    def __init__(self, n_qubits: int, n_features: int, config: dict):
        self.n_qubits = n_qubits
        self.n_features = n_features
        self.fpq = max(1, n_features // n_qubits)

        # LLM-generated parameters
        self.ry1 = config.get('ry1_scale', 1.4)
        self.rz1 = config.get('rz1_scale', 0.6)
        self.rx = config.get('rx_scale', 1.0)
        self.ry2 = config.get('ry2_scale', 1.1)
        self.rz2 = config.get('rz2_scale', 0.5)
        self.pattern = config.get('entangle_pattern', 'circular')
        self.shift = config.get('feature_shift', 3)

    def _entangle(self, qc: QuantumCircuit):
        """Apply entanglement pattern."""
        n = self.n_qubits
        if self.pattern == "linear":
            for i in range(n - 1):
                qc.cx(i, i + 1)
        elif self.pattern == "circular":
            for i in range(n - 1):
                qc.cx(i, i + 1)
            if n > 2:
                qc.cx(n - 1, 0)
        elif self.pattern == "full":
            for i in range(n):
                for j in range(i + 1, min(i + 3, n)):
                    qc.cz(i, j)

    def build_circuit(self, x: np.ndarray) -> QuantumCircuit:
        """Build 5-layer quantum circuit."""
        qc = QuantumCircuit(self.n_qubits)
        n = self.n_qubits
        nf = self.n_features
        fpq = self.fpq

        # Layer 1: RY primary encoding (intensity)
        for i in range(n):
            s, e = i * fpq, min((i + 1) * fpq, nf)
            if s < nf:
                theta = np.pi * self.ry1 * np.mean(x[s:e])
                qc.ry(theta, i)

        # Layer 2: RZ product encoding (correlations)
        for i in range(n):
            idx1 = (i * fpq) % nf
            idx2 = ((i + 1) * fpq) % nf
            theta = np.pi * self.rz1 * x[idx1] * x[idx2]
            qc.rz(theta, i)

        # Entanglement block 1
        self._entangle(qc)

        # Layer 3: RX encoding (variance/texture)
        for i in range(n):
            s, e = i * fpq, min((i + 1) * fpq, nf)
            if s < nf:
                theta = np.pi * self.rx * np.std(x[s:e])
                qc.rx(theta, i)

        # Layer 4: RY re-upload (shifted - captures edges)
        for i in range(n):
            s = ((i + self.shift) * fpq) % nf
            e = min(s + fpq, nf)
            if s < nf:
                theta = np.pi * self.ry2 * np.mean(x[s:e])
                qc.ry(theta, i)

        # Entanglement block 2
        self._entangle(qc)

        # Layer 5: RZ final (range encoding)
        for i in range(n):
            s, e = i * fpq, min((i + 1) * fpq, nf)
            if s < nf:
                theta = np.pi * self.rz2 * (np.max(x[s:e]) - np.min(x[s:e]))
                qc.rz(theta, i)

        return qc

    def extract_features(self, x: np.ndarray) -> np.ndarray:
        """Extract quantum probability features."""
        circuit = self.build_circuit(x)
        sv = Statevector.from_instruction(circuit)
        probs = np.abs(sv.data) ** 2
        return probs

    def transform(self, X: np.ndarray) -> np.ndarray:
        """Transform dataset."""
        return np.array([self.extract_features(x) for x in X])

print("AQEDQuantumFeatureMap ready!")

"""## AQED-Hybrid Classifier for Fashion-MNIST"""

class AQEDHybridClassifier:
    """
    AQED-Hybrid Classifier optimized for Fashion-MNIST.
    """

    def __init__(self, n_qubits: int, n_features: int, config: dict,
                 quantum_weight: float = 0.3, n_quantum_features: int = 120):
        self.n_qubits = n_qubits
        self.n_features = n_features
        self.quantum_weight = quantum_weight
        self.n_quantum_features = n_quantum_features

        self.qfm = AQEDQuantumFeatureMap(n_qubits, n_features, config)
        self.q_selector = None
        self.q_scaler = StandardScaler()
        self.c_scaler = StandardScaler()
        self.classifier = None

    def fit(self, X: np.ndarray, y: np.ndarray, verbose: bool = True):
        """Fit the classifier."""
        if verbose:
            print(f"\nStep 1: Quantum Feature Extraction ({len(X)} samples)...")

        Q_raw = self.qfm.transform(X)
        if verbose:
            print(f"  Raw quantum features: {Q_raw.shape}")

        # Select best quantum features
        if verbose:
            print(f"\nStep 2: Selecting top {self.n_quantum_features} quantum features...")

        self.q_selector = SelectKBest(mutual_info_classif, k=min(self.n_quantum_features, Q_raw.shape[1]))
        Q_selected = self.q_selector.fit_transform(Q_raw, y)

        if verbose:
            print(f"  Selected quantum features: {Q_selected.shape}")

        # Scale and weight features
        Q_scaled = self.q_scaler.fit_transform(Q_selected)
        C_scaled = self.c_scaler.fit_transform(X)

        Q_weighted = Q_scaled * self.quantum_weight
        C_weighted = C_scaled * (1 - self.quantum_weight)

        hybrid_features = np.hstack([C_weighted, Q_weighted])

        if verbose:
            print(f"\nStep 3: Hybrid features: {hybrid_features.shape}")

        # Train classifier
        if verbose:
            print(f"\nStep 4: Training SVM classifier...")

        self.classifier = SVC(kernel='rbf', C=10, gamma='scale')
        self.classifier.fit(hybrid_features, y)

        train_acc = self.classifier.score(hybrid_features, y)
        if verbose:
            print(f"  Training accuracy: {train_acc*100:.2f}%")

        return self

    def _transform(self, X: np.ndarray) -> np.ndarray:
        Q_raw = self.qfm.transform(X)
        Q_selected = self.q_selector.transform(Q_raw)
        Q_scaled = self.q_scaler.transform(Q_selected)
        C_scaled = self.c_scaler.transform(X)

        Q_weighted = Q_scaled * self.quantum_weight
        C_weighted = C_scaled * (1 - self.quantum_weight)

        return np.hstack([C_weighted, Q_weighted])

    def predict(self, X: np.ndarray) -> np.ndarray:
        return self.classifier.predict(self._transform(X))

    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        return accuracy_score(y, self.predict(X))

print("AQEDHybridClassifier ready!")

"""## Load Fashion-MNIST Data"""

def load_fashion_mnist(n_samples: int = 10000, n_pca: int = 50, test_size: float = 0.2, seed: int = 42):
    """Load and preprocess Fashion-MNIST."""
    print("Loading Fashion-MNIST...")
    fmnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False, parser='auto')
    X, y = fmnist.data, fmnist.target.astype(int)

    # Class names for reference
    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
    print(f"Classes: {class_names}")

    # Sample
    rng = np.random.RandomState(seed)
    idx = rng.choice(len(X), min(n_samples, len(X)), replace=False)
    X, y = X[idx], y[idx]

    # PCA
    print(f"Applying PCA ({n_pca} components)...")
    pca = PCA(n_components=n_pca, random_state=seed)
    X_pca = pca.fit_transform(X)
    print(f"Variance explained: {sum(pca.explained_variance_ratio_):.3f}")

    # Normalize
    scaler = MinMaxScaler()
    X_norm = scaler.fit_transform(X_pca)

    return train_test_split(X_norm, y, test_size=test_size, random_state=seed, stratify=y), class_names

print("Data loader ready!")

"""## Run Experiment"""

# Configuration for Fashion-MNIST
N_SAMPLES = 10000
N_PCA = 80
N_QUBITS = 10
QUANTUM_WEIGHT = 0.25
N_Q_FEATURES = 120

print("="*60)
print("AQED-HYBRID FOR FASHION-MNIST")
print("="*60)
print(f"\nConfiguration:")
print(f"  Samples: {N_SAMPLES}")
print(f"  PCA: {N_PCA}")
print(f"  Qubits: {N_QUBITS}")
print(f"  Quantum weight: {QUANTUM_WEIGHT}")
print(f"  Target accuracy: 85%")

# Load data
(X_train, X_test, y_train, y_test), class_names = load_fashion_mnist(
    n_samples=N_SAMPLES,
    n_pca=N_PCA
)

print(f"\nData ready:")
print(f"  Train: {X_train.shape}")
print(f"  Test: {X_test.shape}")

# Generate config via Claude API
print("\n" + "="*60)
print("STEP 1: Generate Config via Claude API")
print("="*60)

designer = ClaudeQuantumDesigner(CLAUDE_API_KEY)
config = designer.generate_config(N_QUBITS, N_PCA, "Fashion-MNIST")

print(f"\nConfig: {config}")

# Train
print("\n" + "="*60)
print("STEP 2: Train AQED-Hybrid")
print("="*60)

start = time.time()

hybrid = AQEDHybridClassifier(
    n_qubits=N_QUBITS,
    n_features=N_PCA,
    config=config,
    quantum_weight=QUANTUM_WEIGHT,
    n_quantum_features=N_Q_FEATURES
)

hybrid.fit(X_train, y_train)

print(f"\nTraining time: {time.time() - start:.1f}s")

# Evaluate
print("\n" + "="*60)
print("STEP 3: Evaluate")
print("="*60)

print("\nExtracting test features...")
y_pred = hybrid.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n{'='*50}")
print(f"AQED-HYBRID ACCURACY: {accuracy*100:.2f}%")
print(f"TARGET: 85%")
print(f"{'='*50}")

if accuracy >= 0.85:
    print("\n✅ SUCCESS: Achieved target accuracy!")
else:
    print(f"\n⚠️ Current: {accuracy*100:.2f}%")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=class_names, digits=4))

classical = SVC(kernel='rbf', C=10, gamma='scale')
classical.fit(X_train, y_train)
classical_acc = classical.score(X_test, y_test)

print(f"Classical SVM:     {classical_acc*100:.2f}%")
print(f"AQED-Hybrid:       {accuracy*100:.2f}%")
print(f"Difference:        {(accuracy - classical_acc)*100:+.2f}%")

# Hyperparameter tuning
best_acc = 0
best_params = {}

for qw in [0.15, 0.2, 0.25, 0.3, 0.35]:
    for nqf in [80, 100, 120, 150]:
        h = AQEDHybridClassifier(
            n_qubits=N_QUBITS,
            n_features=N_PCA,
            config=config,
            quantum_weight=qw,
            n_quantum_features=nqf
        )
        h.fit(X_train, y_train, verbose=False)
        acc = h.score(X_test, y_test)

        if acc > best_acc:
            best_acc = acc
            best_params = {'quantum_weight': qw, 'n_quantum_features': nqf}
            print(f"  New best: qw={qw}, nqf={nqf} -> {acc*100:.2f}%")

print(f"\n Best: {best_params} -> {best_acc*100:.2f}%")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

# Publication style
plt.rcParams.update({
    'font.family': 'serif',
    'font.size': 11,
    'axes.labelsize': 12,
    'axes.titlesize': 13,
    'figure.dpi': 150,
    'savefig.dpi': 300,
})

# Get metrics
cm = confusion_matrix(y_test, y_pred)
precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)

# Train Random Forest for comparison (if not already done)
print("Training Random Forest baseline...")
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
rf_acc = rf.score(X_test, y_test)
print(f"Random Forest accuracy: {rf_acc*100:.2f}%")

# Generate quantum weight sweep data (from your hyperparameter tuning)
quantum_weights = [0.15, 0.2, 0.25, 0.3, 0.35]
weight_accuracies = []
print("\nCollecting quantum weight accuracies...")
for qw in quantum_weights:
    h = AQEDHybridClassifier(
        n_qubits=N_QUBITS,
        n_features=N_PCA,
        config=config,
        quantum_weight=qw,
        n_quantum_features=N_Q_FEATURES
    )
    h.fit(X_train, y_train, verbose=False)
    acc = h.score(X_test, y_test) * 100
    weight_accuracies.append(acc)
    print(f"  qw={qw}: {acc:.2f}%")

# GRAPH 1: ACCURACY TRAJECTORY + QUANTUM KERNEL MATRIX
fig1, axes = plt.subplots(1, 2, figsize=(12, 4.5))

# Left: Accuracy vs Quantum Weight
axes[0].plot(quantum_weights, weight_accuracies,
             'k-o', markersize=7, linewidth=1.5, markerfacecolor='white', markeredgewidth=1.5)
axes[0].set_xlabel('Quantum Weight (α)')
axes[0].set_ylabel('Accuracy (%)')
axes[0].set_title('Fashion-MNIST: Accuracy vs Quantum Weight')
axes[0].grid(True, alpha=0.3)

# Mark best point
best_idx = np.argmax(weight_accuracies)
axes[0].scatter([quantum_weights[best_idx]], [weight_accuracies[best_idx]],
                color='red', s=150, zorder=5, marker='*',
                label=f'Best: {weight_accuracies[best_idx]:.1f}%')
axes[0].axhline(y=85, color='green', linestyle='--', linewidth=1.5, label='85% Target')
axes[0].legend(loc='lower right')

# Right: Quantum Kernel Matrix
n_sample = min(50, len(X_test))
np.random.seed(42)
sample_idx = np.random.choice(len(X_test), n_sample, replace=False)
print("  Computing quantum kernel matrix...")
Q_sample = hybrid.qfm.transform(X_test[sample_idx])

# Compute kernel
K = Q_sample @ Q_sample.T

# Sort by class
sort_idx = np.argsort(y_test[sample_idx])
K_sorted = K[sort_idx][:, sort_idx]

im = axes[1].imshow(K_sorted, cmap='bwr', aspect='auto')
axes[1].set_xlabel('Sample Index (sorted by class)')
axes[1].set_ylabel('Sample Index (sorted by class)')
axes[1].set_title('Fashion-MNIST: Quantum Kernel Matrix')
plt.colorbar(im, ax=axes[1], shrink=0.8)

plt.tight_layout()
plt.savefig('graph1_fmnist_training_kernel.png', dpi=300, bbox_inches='tight')
plt.savefig('graph1_fmnist_training_kernel.pdf', bbox_inches='tight')
plt.show()

# GRAPH 2: CLASSIFICATION SCATTER WITH MISCLASSIFICATIONS
fig2, ax = plt.subplots(figsize=(10, 8))

# Reduce to 2D
pca_2d = PCA(n_components=2)
X_test_2d = pca_2d.fit_transform(X_test)

# Plot each class with class names
colors = plt.cm.tab10(np.linspace(0, 1, 10))
for cls in range(10):
    mask = y_test == cls
    ax.scatter(X_test_2d[mask, 0], X_test_2d[mask, 1],
               c=[colors[cls]], label=class_names[cls],
               alpha=0.6, s=25, edgecolors='none')

# Highlight misclassified
misclassified = y_test != y_pred
ax.scatter(X_test_2d[misclassified, 0], X_test_2d[misclassified, 1],
           facecolors='none', edgecolors='red', s=120, linewidths=2,
           label=f'Misclassified ({misclassified.sum()})')

ax.set_xlabel('Principal Component #0')
ax.set_ylabel('Principal Component #1')
ax.set_title(f'Fashion-MNIST: Classification Results (Accuracy: {accuracy*100:.1f}%)\nRed circles = misclassified samples')
ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('graph2_fmnist_classification_scatter.png', dpi=300, bbox_inches='tight')
plt.savefig('graph2_fmnist_classification_scatter.pdf', bbox_inches='tight')
plt.show()

# GRAPH 3: QUANTUM VS CLASSICAL FEATURE PROJECTION
fig3, axes = plt.subplots(1, 2, figsize=(14, 5.5))

# Sample for speed
n_train_sample = min(200, len(X_train))
n_test_sample = min(100, len(X_test))

np.random.seed(42)
train_idx = np.random.choice(len(X_train), n_train_sample, replace=False)
test_idx = np.random.choice(len(X_test), n_test_sample, replace=False)

# Get quantum features
print("  Extracting quantum features for visualization...")
Q_train = hybrid.qfm.transform(X_train[train_idx])
Q_test = hybrid.qfm.transform(X_test[test_idx])

# PCA on quantum features
pca_q = PCA(n_components=2)
Q_train_2d = pca_q.fit_transform(Q_train)
Q_test_2d = pca_q.transform(Q_test)

# PCA on classical features
pca_c = PCA(n_components=2)
C_train_2d = pca_c.fit_transform(X_train[train_idx])
C_test_2d = pca_c.transform(X_test[test_idx])

# LEFT: Quantum projection
for cls in range(10):
    mask_train = y_train[train_idx] == cls
    axes[0].scatter(Q_train_2d[mask_train, 0], Q_train_2d[mask_train, 1],
                   c=[plt.cm.tab10(cls/10)], marker='o', s=40, alpha=0.7,
                   edgecolors='white', linewidths=0.5, label=class_names[cls] if cls < 5 else None)
    mask_test = y_test[test_idx] == cls
    axes[0].scatter(Q_test_2d[mask_test, 0], Q_test_2d[mask_test, 1],
                   facecolors='none', edgecolors=plt.cm.tab10(cls/10),
                   marker='s', s=50, linewidths=1.5)

axes[0].set_xlabel('Principal Component #0')
axes[0].set_ylabel('Principal Component #1')
axes[0].set_title('Projection using AQED Quantum Features\n(Filled=Train, Hollow=Test)')
axes[0].grid(True, alpha=0.3)
axes[0].legend(loc='upper right', fontsize=7)

# RIGHT: Classical projection
for cls in range(10):
    mask_train = y_train[train_idx] == cls
    axes[1].scatter(C_train_2d[mask_train, 0], C_train_2d[mask_train, 1],
                   c=[plt.cm.tab10(cls/10)], marker='o', s=40, alpha=0.7,
                   edgecolors='white', linewidths=0.5, label=class_names[cls] if cls >= 5 else None)
    mask_test = y_test[test_idx] == cls
    axes[1].scatter(C_test_2d[mask_test, 0], C_test_2d[mask_test, 1],
                   facecolors='none', edgecolors=plt.cm.tab10(cls/10),
                   marker='s', s=50, linewidths=1.5)

axes[1].set_xlabel('Principal Component #0')
axes[1].set_ylabel('Principal Component #1')
axes[1].set_title('Projection using Classical PCA\n(Filled=Train, Hollow=Test)')
axes[1].grid(True, alpha=0.3)
axes[1].legend(loc='upper right', fontsize=7)

plt.suptitle('Fashion-MNIST: Quantum vs Classical Feature Space Comparison', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('graph3_fmnist_feature_projection.png', dpi=300, bbox_inches='tight')
plt.savefig('graph3_fmnist_feature_projection.pdf', bbox_inches='tight')
plt.show()

# GRAPH 4: COMPREHENSIVE 4-PANEL RESULTS SUMMARY
fig4 = plt.figure(figsize=(14, 11))

# (a) Model comparison
ax1 = fig4.add_subplot(2, 2, 1)
models = ['Classical\nSVM', 'Random\nForest', 'AQED-Hybrid\n(Ours)']
accs = [classical_acc*100, rf_acc*100, accuracy*100]
colors_bar = ['#7f7f7f', '#7f7f7f', '#d62728']
bars = ax1.bar(models, accs, color=colors_bar, edgecolor='black', linewidth=1)
ax1.axhline(y=85, color='green', linestyle='--', linewidth=2, label='85% Target')
ax1.set_ylabel('Test Accuracy (%)')
ax1.set_title('(a) Model Comparison', fontweight='bold')
ax1.set_ylim(min(accs)-3, max(accs)+3)
ax1.legend(loc='upper left')
for bar, acc in zip(bars, accs):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,
             f'{acc:.2f}%', ha='center', fontsize=11, fontweight='bold')
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

# (b) Confusion matrix with class names
ax2 = fig4.add_subplot(2, 2, 2)
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=ax2,
            xticklabels=[c[:6] for c in class_names],  # Truncate names
            yticklabels=[c[:6] for c in class_names],
            cbar_kws={'shrink': 0.8}, annot_kws={'size': 7})
ax2.set_xlabel('Predicted Label')
ax2.set_ylabel('True Label')
ax2.set_title(f'(b) Normalized Confusion Matrix\n(Accuracy: {accuracy*100:.2f}%)', fontweight='bold')
plt.setp(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=8)
plt.setp(ax2.get_yticklabels(), rotation=0, fontsize=8)

# (c) Per-class F1 scores
ax3 = fig4.add_subplot(2, 2, 3)
x = np.arange(10)
colors_f1 = plt.cm.tab10(np.linspace(0, 1, 10))
bars_f1 = ax3.bar(x, f1, color=colors_f1, edgecolor='black', linewidth=0.5)
ax3.set_xlabel('Class')
ax3.set_ylabel('F1-Score')
ax3.set_title('(c) Per-Class F1 Scores', fontweight='bold')
ax3.set_xticks(x)
ax3.set_xticklabels([c[:6] for c in class_names], rotation=45, ha='right', fontsize=8)
ax3.axhline(y=0.85, color='green', linestyle='--', alpha=0.7, label='85% Target')
ax3.set_ylim(0, 1.05)
ax3.legend(loc='lower right')
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# (d) Quantum weight optimization
ax4 = fig4.add_subplot(2, 2, 4)
ax4.plot(quantum_weights, weight_accuracies, 'o-', color='#1f77b4',
         linewidth=2, markersize=8, markerfacecolor='white', markeredgewidth=2)
best_idx = np.argmax(weight_accuracies)
ax4.scatter([quantum_weights[best_idx]], [weight_accuracies[best_idx]],
            color='#d62728', s=200, zorder=5, marker='*', edgecolors='black',
            label=f'Best: {weight_accuracies[best_idx]:.2f}%')
ax4.axhline(y=85, color='green', linestyle='--', linewidth=1.5, label='85% Target')
ax4.axhline(y=classical_acc*100, color='gray', linestyle=':', linewidth=1.5,
            label=f'Classical SVM ({classical_acc*100:.1f}%)')
ax4.set_xlabel('Quantum Weight (α)')
ax4.set_ylabel('Test Accuracy (%)')
ax4.set_title('(d) Quantum Weight Optimization', fontweight='bold')
ax4.legend(loc='lower right', fontsize=9)
ax4.grid(True, alpha=0.3)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)

plt.suptitle(f'AQED-Hybrid Results on Fashion-MNIST (N={len(X_train)+len(X_test):,} samples)',
             fontsize=16, fontweight='bold', y=1.01)
plt.tight_layout()
plt.savefig('graph4_fmnist_comprehensive.png', dpi=300, bbox_inches='tight')
plt.savefig('graph4_fmnist_comprehensive.pdf', bbox_inches='tight')
plt.show()

# PRINT SUMMARY
print("ALL GRAPHS GENERATED SUCCESSFULLY!")

# Per-class analysis
print(f"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
print("-"*50)
for i, name in enumerate(class_names):
    print(f"{name:<15} {precision[i]:.4f}       {recall[i]:.4f}       {f1[i]:.4f}")

# Find hardest and easiest classes
easiest = class_names[np.argmax(f1)]
hardest = class_names[np.argmin(f1)]
print(f"\nEasiest class: {easiest} (F1: {np.max(f1):.4f})")
print(f"Hardest class: {hardest} (F1: {np.min(f1):.4f})")